"""
Server Logic - Advanced FNOL Claims Intelligence System
Handles all backend processing, predictions, and business logic
"""
from shiny import reactive, render, ui
from shinywidgets import render_widget
from sklearn.metrics import r2_score
import pandas as pd
import numpy as np
from pathlib import Path
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime
import shap
import pickle
from typing import Dict, List, Optional
import pins 
from vetiver import VetiverModel
# import effector


from global_py import (
    APP_CONFIG, MODEL_CONFIG, BUSINESS_THRESHOLDS,
    format_currency, format_percentage, calculate_severity_level,
    calculate_reserve_recommendation, calculate_business_metrics,
    generate_actionable_insights, create_severity_gauge,
    validate_input_data, calculate_roi_metrics,
    preprocess_claim_data, FEATURE_BUSINESS_CONTEXT,
    get_modeling_data, run_predictions, load_model
)

from src.utils.config_loader import ConfigLoader
from src.data.data_loader import InsuranceDataLoader
from src.data.preprocessing import DataPreprocessor

config_loader = ConfigLoader()
config = config_loader.get_main_config()
feature_config = config_loader.load_config("feature_config.yaml")
data_loader = InsuranceDataLoader(config)
preprocessor = DataPreprocessor(config, feature_config)
    
numeric_features = feature_config.get('numeric_features', [])
categorical_features = feature_config.get('categorical_features', [])

# ============================================================================
# SERVER FUNCTION
# ============================================================================

def server(input, output, session):
    
    # ========================================================================
    # REACTIVE VALUES AND STATE MANAGEMENT
    # ========================================================================
    
    # Store application state
    app_state = reactive.Value({
        "current_model": None,
        "model_metadata": {},
        "predictions_history": [],
        "batch_data": None,
        "batch_predictions": None,
        "model_loaded": False,
        "vetiver_pin": None
    })
    
    uploaded_data = reactive.Value(None)
    current_prediction = reactive.Value(None)
    batch_results = reactive.Value(None)
    
    model_link = load_model("victoroseji/fnol_claim_model","claims_model_pins_board")
    data_link = load_model("victoroseji/fnol_claims_dataset","data")
    ### =================================================================
    @reactive.Effect
    def _():                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
        # Read the CSV and set the reactive value
        try:
            # 1. Read the file
            data_path = data_link + "/processed"
            board = pins.board_folder(data_path)

            claims_data = board.pin_read("processed_claims")
                        
            # --- START ONE-TIME PREDICTION PROCESS ---
            # 2. Assuming numeric_features/categorical_features are defined globally or from config
            features_for_model = get_modeling_data(claims_data, numeric_features, categorical_features)
            
            # 3. Get Predictions
            preds = run_predictions(features_for_model, model_link,"claims_model_best")
            
            # 4. Enrich the ORIGINAL data with results
            claims_data["Predictions"] = np.expm1(preds)
            
            # 5. Add Severity and Recommendation
            claims_data["Severity"] = claims_data["Predictions"].apply(
                lambda x: calculate_severity_level(x)[0]
            )
            claims_data["Recommended_Reserve"] = claims_data["Predictions"].apply(
                calculate_reserve_recommendation
            )
            # 6. Load the processed DataFrame into the reactive value
            uploaded_data.set(claims_data)
            
        except Exception as e:
            print(f"Error loading or processing CSV: {e}")

    ### ===========================================================
    @reactive.calc
    def batch_results():
        date_range = input.dashboard_date_range()
        df = uploaded_data.get()
        
        if df is None:
            return None
        
        # Filter logic
        mask = (df["Accident_Date"].dt.date >= date_range[0]) & \
            (df["Accident_Date"].dt.date <= date_range[1])
        
        # Just return the dataframe. Shiny handles the "setting" internally.
        return df.loc[mask]
    ### ===============================================================
    

    # ========================================================================
    # DASHBOARD TAB - REACTIVE OUTPUTS
    # ========================================================================
    
    @output
    @render.text
    def system_status():
        """Display system operational status"""
        if app_state()["model_loaded"]:
            return "ðŸŸ¢ Operational"
        return "ðŸŸ¡ No Model Loaded"
    
    @output
    @render.text
    def models_count():
        """Display number of models available"""
        return f"{len(MODEL_CONFIG['available_models'])} Available"
    
    @output
    @render.text
    def total_claims_processed():
        """Total claims in current dataset"""
        data = batch_results()
        if data is not None and not data.empty:
            return f"{len(data):,}"
        return "0"
    
    @output
    @render.text
    def total_exposure_amount():
        """Total financial exposure"""
        results = batch_results()
        if results is not None and "Ultimate_Claim_Amount" in results.columns:
            total = results["Ultimate_Claim_Amount"].sum()
            return format_currency(total)
        return "Â£0.00"
    
    @output
    @render.text
    def average_claim_value():
        """Average claim value"""
        results = batch_results()
        if results is not None and "Ultimate_Claim_Amount" in results.columns:
            avg = results["Ultimate_Claim_Amount"].mean()
            return format_currency(avg)
        return "Â£0.00"
    
    @output
    @render.text
    def high_severity_count():
        """Count of high severity claims"""
        results = batch_results()
        if results is not None and "Ultimate_Claim_Amount" in results.columns:
            high_severity = results[
                results["Ultimate_Claim_Amount"] >= BUSINESS_THRESHOLDS["high_severity"]
            ]
            return f"{len(high_severity):,}"
        return "0"
    
    @output
    @render_widget
    def severity_distribution_plot():
        """Plot severity distribution"""
        results = batch_results()
        if results is None or "Ultimate_Claim_Amount" not in results.columns:
            return None
        
        # Categorize by severity
        results["Severity"] = results["Ultimate_Claim_Amount"].apply(
            lambda x: calculate_severity_level(x)[0]
        )
        
        severity_counts = results["Severity"].value_counts()
        
        fig = px.bar(
            x=severity_counts.index,
            y=severity_counts.values,
            labels={"x": "Severity Level", "y": "Count"},
            title="Claims by Severity Level",
            color=severity_counts.index,
            color_discrete_map={
                "Low": APP_CONFIG["success_color"],
                "Medium": APP_CONFIG["info_color"],
                "High": APP_CONFIG["warning_color"],
                "Critical": APP_CONFIG["danger_color"]
            }
        )
        
        fig.update_layout(showlegend=False, height=350)
        return fig
    
    @output
    @render_widget
    def prediction_accuracy_plot():
        """Compare predicted vs estimated amounts"""
        results = batch_results()
        if results is None or "Ultimate_Claim_Amount" not in results.columns:
            return None
        
        if "Estimated_Claim_Amount" not in results.columns:
            return None
        
        fig = px.scatter(
            results,
            x="Estimated_Claim_Amount",
            y="Ultimate_Claim_Amount",
            title="Predicted vs. Initial Estimate",
            labels={
                "Estimated_Claim_Amount": "Initial Estimate (Â£)",
                "Ultimate_Claim_Amount": "Predicted Amount (Â£)"
            },
            opacity=0.6
        )
        
        # Add diagonal line
        max_val = max(results["Estimated_Claim_Amount"].max(), 
                      results["Ultimate_Claim_Amount"].max())
        fig.add_trace(go.Scatter(
            x=[0, max_val],
            y=[0, max_val],
            mode="lines",
            line=dict(dash="dash", color="red"),
            name="Perfect Prediction"
        ))
        
        fig.update_layout(height=350)
        return fig
    
    @output
    @render.ui
    def business_insights_dashboard():
        """Generate business insights from current data"""
        results = batch_results()
        if results is None or results.empty:
            return ui.div(
                ui.tags.p("Upload claims data to view business insights.", 
                         style="text-align: center; color: #666; padding: 40px;")
            )
        
        metrics = calculate_business_metrics(results)
        roi = calculate_roi_metrics(results)
        
        insights = []
        
        # High severity alert
        if "Critical" in metrics.get("severity_distribution", {}):
            critical_count = metrics["severity_distribution"]["Critical"]
            insights.append(
                ui.div(
                    {"class": "insight-card alert-high"},
                    ui.div("âš ï¸ Critical Claims Alert", {"class": "insight-title"}),
                    ui.p(f"{critical_count} claims exceed critical threshold (Â£{BUSINESS_THRESHOLDS['critical_severity']:,}). Immediate review recommended.")
                )
            )
        
        # Cost savings insight
        if roi.get("total_savings", 0) > 0:
            insights.append(
                ui.div(
                    {"class": "insight-card alert-low"},
                    ui.div("ðŸ’° Projected Cost Savings", {"class": "insight-title"}),
                    ui.p(f"ML-driven optimization projects {format_currency(roi['total_savings'])} in savings through improved reserve accuracy and fraud detection.")
                )
            )
        
        # Average claim trend
        avg_claim = metrics.get("average_claim", 0)
        if avg_claim > 20000:
            insights.append(
                ui.div(
                    {"class": "insight-card alert-medium"},
                    ui.div("ðŸ“Š Elevated Average Claim Cost", {"class": "insight-title"}),
                    ui.p(f"Average claim ({format_currency(avg_claim)}) exceeds industry benchmarks. Consider enhanced fraud detection protocols.")
                )
            )
        
        return ui.div(*insights) if insights else ui.p("No significant insights at this time.")
    
    @output
    @render_widget
    def claims_by_type_plot():
        """Distribution of claims by type"""
        results = batch_results()
        if results is None or "Claim_Type" not in results.columns:
            return None
        
        type_counts = results["Claim_Type"].value_counts()
        
        fig = px.pie(
            values=type_counts.values,
            names=type_counts.index,
            title="Claims Distribution by Type",
            hole=0.4
        )
        
        fig.update_layout(height=350)
        return fig
    
    @output
    @render_widget
    def roi_metrics_plot():
        """Display ROI metrics"""
        results = batch_results()
        if results is None or results.empty:
            return None
        
        roi = calculate_roi_metrics(results)
        
        if not roi:
            return None
        
        categories = ["Reserve\nOptimization", "Fraud\nPrevention", "Operational\nEfficiency"]
        values = [
            roi.get("reserve_optimization", 0),
            roi.get("fraud_prevention", 0),
            roi.get("operational_efficiency", 0)
        ]
        
        fig = go.Figure(data=[
            go.Bar(
                x=categories,
                y=values,
                marker_color=[APP_CONFIG["success_color"], APP_CONFIG["info_color"], APP_CONFIG["warning_color"]],
                text=[format_currency(v) for v in values],
                textposition="outside"
            )
        ])
        
        fig.update_layout(
            title="Projected Cost Savings by Category",
            yaxis_title="Savings (Â£)",
            height=350,
            showlegend=False
        )
        
        return fig
    
    # ========================================================================
    # SINGLE CLAIM PREDICTION TAB
    # ========================================================================
    split_path = data_link + "/interim"
    data_board = pins.board_folder(split_path, allow_pickle_read=True)
    model_board = pins.board_folder(model_link, allow_pickle_read=True)

    # board = pins.board_folder(board_name, allow_pickle_read=True)
    train_data = data_board.pin_read("train_test_data_split")["X_train"]
    test_data = data_board.pin_read("train_test_data_split")["X_test"]
    v = VetiverModel.from_pin(model_board, "claims_model_best")
    final_model = v.model

    # Preprocess the instance for SHAP
    preprocessed_test = final_model.named_steps["preprocessing"].transform(test_data)
    preprocessed_train = final_model.named_steps["preprocessing"].transform(train_data)

    # To explain in the original scale, use KernelExplainer with a wrapped model that includes the np.expm1 transformation
    # This provides approximate SHAP values that are additive in the transformed (original) space
    def wrapped_model(X):
        return np.expm1(final_model.named_steps["model"].predict(X))

    explainer = shap.KernelExplainer(wrapped_model, preprocessed_train)

    @reactive.Effect
    @reactive.event(input.predict_btn)
    def _():
        """Generate prediction for single claim"""
        # 1. Select one data instance as a baseline
        master_df = uploaded_data.get()
        if master_df is None:
            return
            
        # Take the first row (or a specific row) to use as a template
        # We use .iloc[[0]] to keep it as a DataFrame
        single_row = master_df.iloc[[0]].copy()
        # 2. Update the template with user-collected features
        features = {
            "Estimated_Claim_Amount": input.estimated_amount(),
            "Driver_Age": input.driver_age(),
            "License_Age": input.license_age(),
            "Vehicle_Age": input.vehicle_age(),
            "Claim_Type": input.claim_type(),
            "Weather_Condition": input.weather_condition(),
            "Traffic_Condition": input.traffic_condition(),
            "FNOL_Delay_Hours": input.fnol_delay_hours()
        }

        for key, value in features.items():
            single_row[key] = value

        # 3. Calculate Derived Metrics (Recalculate logic on the new inputs)
        single_row['High_Risk_Driver'] = np.where((single_row['Driver_Age'] < 25) | (single_row['Driver_Age'] > 70), 'Yes', 'No')
        single_row['Inexperienced_Driver'] = np.where(single_row['License_Age'] < 2, 'Yes', 'No')
        single_row['Old_Vehicle'] = np.where(single_row['Vehicle_Age'] > 10, 'Yes', 'No')
        single_row['Early_FNOL'] = np.where(single_row['FNOL_Delay'] <= 1, 'Yes', 'No')
        
        # Use the function we created earlier to transform this single row
        transformed_row = get_modeling_data(single_row, numeric_features, categorical_features)
            
        # 3. Get Predictions
        single_preds = run_predictions(transformed_row, model_link,"claims_model_best")
        single_preds = np.expm1(single_preds)
        
        # Assuming calculate_severity_level returns (label, color)
        severity, severity_color = calculate_severity_level(single_preds[0])
        
        prediction_result = {
            "claim_id": input.claim_id(),
            "policy_id": input.policy_id(),
            "predicted_amount": single_preds[0],
            "features": features,
            "severity": severity,
            "severity_color": severity_color,
            "reserve_recommendation": calculate_reserve_recommendation(single_preds[0]),
            "insights": generate_actionable_insights(single_preds[0], features),
            "timestamp": datetime.now()
        }
        
        current_prediction.set(prediction_result)
        
        # Update history
        state = app_state()
        state["predictions_history"].append(prediction_result)
        app_state.set(state)
    
    @output
    @render.text
    def prediction_amount():
        """Display predicted amount"""
        pred = current_prediction()
        if pred:
            return format_currency(pred["predicted_amount"])
        return "â€”"
    
    @output
    @render.text
    def prediction_severity():
        """Display severity level"""
        pred = current_prediction()
        if pred:
            return pred["severity"]
        return "â€”"
    
    @output
    @render.text
    def recommended_reserve():
        """Display recommended reserve"""
        pred = current_prediction()
        if pred:
            return format_currency(pred["reserve_recommendation"])
        return "â€”"
    
    @output
    @render.ui
    def severity_gauge_plot():
        """Render severity gauge"""
        pred = current_prediction()
        if not pred:
            return ui.div("Generate a prediction to view severity gauge", 
                         style="text-align: center; padding: 40px; color: #666;")
        
        fig = create_severity_gauge(pred["predicted_amount"])
        return ui.HTML(fig.to_html(include_plotlyjs="cdn", div_id="gauge"))
    
    @output
    @render_widget
    def shap_plot():
        """Generate SHAP waterfall plot"""
        # Compute SHAP values for the instance
        shap_values = explainer(preprocessed_test)

        # Extract SHAP components (assuming single output/regression)
        shap_value = shap_values[0]  # SHAP Explanation object for the instance
        base_value = shap_value.base_values  # Expected value
        feature_values = shap_value.values  # SHAP contributions per feature

        # Get feature names from the preprocessor
        feature_names = final_model.named_steps["preprocessing"].get_feature_names_out()

        # Sort features by absolute SHAP value descending for better visualization
        indices = np.argsort(-np.abs(feature_values))
        sorted_feature_names = feature_names[indices]
        sorted_feature_values = feature_values[indices]

        # Compute final prediction from SHAP (approximates the model prediction due to nonlinear transformation)
        final_prediction = base_value + sum(sorted_feature_values)

        # Create Plotly Waterfall chart in original scale
        fig = go.Figure(go.Waterfall(
            name="SHAP Waterfall",
            orientation="h",  # Horizontal for feature names on y-axis
            measure=["absolute"] + ["relative"] * len(sorted_feature_values) + ["total"],
            x=[base_value] + list(sorted_feature_values) + [0],
            y=["Expected Value"] + list(sorted_feature_names) + ["Prediction"],
            textposition="outside",
            text=[f"{base_value:.2f}"] + [f"{v:+.2f}" for v in sorted_feature_values] + [f"{final_prediction:.2f}"],
            connector={"line": {"color": "rgb(63, 63, 63)"}},
        ))

        fig.update_layout(
            title="SHAP Waterfall Plot for Transformed Instance Prediction (Original Scale, Approximate)",
            yaxis_title="Features",
            xaxis_title="SHAP Value Contribution (Original Scale)",
            waterfallgap=0.3,
        )
        
        return fig
    
    @output
    @render.ui
    def actionable_insights():
        """Display actionable business insights"""
        pred = current_prediction()
        if not pred:
            return ui.p("Generate a prediction to view insights")
        
        insights = pred["insights"]
        if not insights:
            return ui.p("No specific insights for this claim")
        
        insight_elements = []
        for insight in insights:
            priority_class = f"alert-{insight['priority'].lower()}"
            
            insight_elements.append(
                ui.div(
                    {"class": f"insight-card {priority_class}"},
                    ui.div(
                        ui.tags.strong(f"{insight['title']} "),
                        ui.tags.span(f"[{insight['priority']} Priority]", 
                                   style="font-size: 0.85rem; opacity: 0.8;")
                    ),
                    ui.p(insight['message']),
                    ui.div(
                        ui.tags.strong("Recommended Action: "),
                        ui.tags.span(insight['action'])
                    )
                )
            )
        
        return ui.div(*insight_elements)
    
    @output
    @render.ui
    def risk_factors_analysis():
        """Analyze risk factors for the claim"""
        pred = current_prediction()
        if not pred:
            return ui.p("Generate a prediction to view risk analysis")
        
        features = pred["features"]
        risk_items = []
        
        # Analyze each feature
        if features["Driver_Age"] < 25:
            risk_items.append("ðŸ”´ Young driver (high risk)")
        elif features["Driver_Age"] > 70:
            risk_items.append("ðŸŸ¡ Senior driver (moderate risk)")
        else:
            risk_items.append("ðŸŸ¢ Driver age in standard range")
        
        if features["Vehicle_Age"] > 10:
            risk_items.append("ðŸŸ¡ Older vehicle (higher repair costs)")
        else:
            risk_items.append("ðŸŸ¢ Newer vehicle")
        
        if features["FNOL_Delay_Hours"] > 24:
            risk_items.append("ðŸ”´ Significant FNOL delay (fraud indicator)")
        else:
            risk_items.append("ðŸŸ¢ Timely FNOL reporting")
        
        if features["Weather_Condition"] in ["Rainy", "Snowy", "Foggy"]:
            risk_items.append("ðŸŸ¡ Adverse weather conditions")
        
        return ui.div(
            *[ui.p(item) for item in risk_items]
        )
    
    @output
    @render.ui
    def recommended_actions():
        """Generate recommended actions"""
        pred = current_prediction()
        if not pred:
            return ui.p("Generate a prediction to view recommendations")
        
        actions = []
        severity = pred["severity"]
        
        if severity in ["High", "Critical"]:
            actions.extend([
                "âœ“ Escalate to senior claims handler",
                "âœ“ Initiate fraud investigation protocol",
                "âœ“ Request detailed damage assessment"
            ])
        
        actions.extend([
            "âœ“ Set reserve to recommended amount",
            "âœ“ Schedule repair assessment within 48 hours",
            "âœ“ Contact customer for additional information"
        ])
        
        return ui.div(
            *[ui.p(action, style="margin: 8px 0;") for action in actions]
        )
    
    # ========================================================================
    # BATCH PREDICTION TAB
    # ========================================================================
    
    @reactive.Effect
    @reactive.event(input.file_upload)
    def handle_file_upload():
        """Handle CSV file upload"""
        file_info = input.file_upload()
        if file_info is None or len(file_info) == 0:
            return
        
        try:
            # Read uploaded CSV
            df = pd.read_csv(file_info[0]["datapath"])

            date_cols = ["Accident_Date", "FNOL_Date", "Settlement_Date", "Date_of_Birth", "Full_License_Issue_Date"]
        
            for col in date_cols:
                if col in df.columns:
                    df[col] = pd.to_datetime(df[col])
            
            # process data for prediction ---
            features_for_model = get_modeling_data(df, numeric_features, categorical_features)
            
            # 4. Get Predictions
            preds = run_predictions(features_for_model, model_link,"claims_model_best")
            
            # 5. Enrich the ORIGINAL data with results
            df["Predictions"] = np.expm1(preds)
            
            # Add Severity and Recommendation
            df["Severity"] = df["Predictions"].apply( lambda x: calculate_severity_level(x)[0] )
            df["Recommended_Reserve"] = df["Predictions"].apply( calculate_reserve_recommendation )
            
            uploaded_data.set(df)
            
            ui.notification_show(
                f"Successfully loaded {len(df)} claims",
                type="message",
                duration=3
            )
        except Exception as e:
            ui.notification_show(
                f"Error loading file: {str(e)}",
                type="error",
                duration=5
            )
    
    @output
    @render.ui
    def validation_status():
        """Display data validation status"""
        data = uploaded_data()
        if data is None:
            return ui.div(
                {"style": "padding: 20px; text-align: center; color: #666;"},
                ui.p("No data uploaded")
            )
        
        is_valid, errors = validate_input_data(data)
        
        if is_valid:
            return ui.div(
                {"style": "padding: 15px; background: #d4edda; border: 1px solid #c3e6cb; border-radius: 5px;"},
                ui.p("âœ“ Data validation passed", style="color: #155724; font-weight: bold; margin: 0;")
            )
        else:
            error_list = ui.tags.ul(*[ui.tags.li(err) for err in errors])
            return ui.div(
                {"style": "padding: 15px; background: #f8d7da; border: 1px solid #f5c6cb; border-radius: 5px;"},
                ui.p("âœ— Validation errors:", style="color: #721c24; font-weight: bold;"),
                error_list
            )
    
    @output
    @render.text
    def uploaded_rows_count():
        """Display count of uploaded rows"""
        data = uploaded_data()
        if data is not None:
            return f"Uploaded: {len(data):,} rows, {len(data.columns)} columns"
        return "No data uploaded"
    
    @reactive.Effect
    @reactive.event(input.process_batch_btn)
    def process_batch_predictions():
        """Process batch predictions"""
        data = uploaded_data()
        if data is None or data.empty:
            ui.notification_show("Please upload data first", type="warning")
            return
        
        try:
            # Simulate batch predictions
            predictions = []
            df = modeling_data()
            
            # 1. Connect to the board (replace with your board type, e.g., board_s3)
            board = pins.board_folder("model_pins_board", allow_pickle_read=True)

            # 2. Retrieve the model by name
            v = VetiverModel.from_pin(board, "random_forest")

            # 3. Apply prediction
            pred = v.model.predict(df)
            
            # 4. Add predictions to dataframe
            results_df = df.copy()
            results_df["Predictions"] = pred
            results_df["Severity"] = results_df["Predictions"].apply(
                lambda x: calculate_severity_level(x)[0]
            )
            results_df["Recommended_Reserve"] = results_df["Predictions"].apply(
                calculate_reserve_recommendation
            )
            
            batch_results.set(results_df)
            
            ui.notification_show(
                f"Successfully processed {len(results_df)} predictions",
                type="message",
                duration=3
            )
        except Exception as e:
            ui.notification_show(
                f"Error processing batch: {str(e)}",
                type="error",
                duration=5
            )
    
    @output
    @render.data_frame
    def batch_results_table():
        """Display batch results"""
        results = batch_results()
        if results is None:
            return None
        
        # Select key columns for display
        display_cols = ["Claim_ID", "Policy_ID", "High_Risk_Driver","Inexperienced_Driver","Estimated_Claim_Amount", 
                       "Ultimate_Claim_Amount", "Severity", "Recommended_Reserve"]
        
        display_df = results[[col for col in display_cols if col in results.columns]].copy()
        
        # Format currency columns
        for col in ["Estimated_Claim_Amount", "Ultimate_Claim_Amount", "Recommended_Reserve"]:
            if col in display_df.columns:
                display_df[col] = display_df[col].apply(lambda x: format_currency(x) if pd.notna(x) else "")
        
        return display_df
    
    @output
    @render_widget
    def batch_statistics_plot():
        """Display batch processing statistics"""
        results = batch_results()
        if results is None or results.empty:
            return None
        
        severity_counts = results["Severity"].value_counts()
        
        fig = go.Figure(data=[
            go.Bar(
                x=severity_counts.index,
                y=severity_counts.values,
                marker_color=[
                    APP_CONFIG["success_color"] if s == "Low" else
                    APP_CONFIG["info_color"] if s == "Medium" else
                    APP_CONFIG["warning_color"] if s == "High" else
                    APP_CONFIG["danger_color"]
                    for s in severity_counts.index
                ],
                text=severity_counts.values,
                textposition="outside"
            )
        ])
        
        fig.update_layout(
            title="Batch Results by Severity",
            xaxis_title="Severity Level",
            yaxis_title="Count",
            height=350,
            showlegend=False
        )
        
        return fig
    
    @output
    @render_widget
    def batch_distribution_plot():
        """Distribution of predicted amounts"""
        results = batch_results()
        if results is None or "Ultimate_Claim_Amount" not in results.columns:
            return None
        
        fig = px.histogram(
            results,
            x="Ultimate_Claim_Amount",
            nbins=30,
            title="Distribution of Predicted Amounts",
            labels={"Ultimate_Claim_Amount": "Predicted Amount (Â£)"}
        )
        
        fig.update_layout(height=350)
        return fig
    
    @output
    @render.download
    def download_predictions():
        """Download batch predictions as CSV"""
        def _():
            results = batch_results()
            if results is not None:
                return results.to_csv(index=False)
            return ""
        
        return _
    
    # ========================================================================
    # MODEL MANAGEMENT TAB
    # ========================================================================
    
    @reactive.Effect
    @reactive.event(input.load_model_btn)
    def load_model_from_huggingface():
        """Load model from HuggingFace using Vetiver"""
        try:
            hf_repo = input.hf_repo()
            pin_name = input.pin_name()
            
            ui.notification_show(
                f"Loading model '{pin_name}' from {hf_repo}...",
                type="message",
                duration=3
            )
            
            # Simulate model loading
            # In production: Use vetiver to load from HuggingFace
            # from vetiver import VetiverModel
            # model = VetiverModel.from_pin(board, pin_name)
            
            state = app_state()
            state["model_loaded"] = True
            state["vetiver_pin"] = pin_name
            state["model_metadata"] = {
                "pin_name": pin_name,
                "repository": hf_repo,
                "loaded_at": datetime.now(),
                "model_type": input.selected_model()
            }
            app_state.set(state)
            
            ui.notification_show(
                "Model loaded successfully!",
                type="message",
                duration=3
            )
        except Exception as e:
            ui.notification_show(
                f"Error loading model: {str(e)}",
                type="error",
                duration=5
            )
    
    @output
    @render.ui
    def model_load_status():
        """Display model loading status"""
        state = app_state()
        if not state["model_loaded"]:
            return ui.div(
                {"style": "padding: 15px; background: #fff3cd; border-radius: 5px;"},
                ui.p("âš  No model loaded", style="margin: 0; color: #856404;")
            )
        
        metadata = state["model_metadata"]
        return ui.div(
            {"style": "padding: 15px; background: #d4edda; border-radius: 5px;"},
            ui.p("âœ“ Model Loaded", style="color: #155724; font-weight: bold;"),
            ui.tags.small(f"Pin: {metadata.get('pin_name', 'N/A')}"),
            ui.br(),
            ui.tags.small(f"Type: {metadata.get('model_type', 'N/A')}"),
            ui.br(),
            ui.tags.small(f"Loaded: {metadata.get('loaded_at', 'N/A')}")
        )
    
    @output
    @render.ui
    def model_performance_metrics():
        """Display model performance metrics"""
        state = app_state()
        if not state["model_loaded"]:
            return ui.p("Load a model to view performance metrics")
        
        # Simulated metrics
        metrics = {
            "RÂ² Score": 0.8432,
            "RMSE": 8460.83,
            "MAE": 4183.94,
            "MAPE": 12.3
        }
        
        metric_cards = []
        colors = [APP_CONFIG["success_color"], APP_CONFIG["info_color"], 
                 APP_CONFIG["warning_color"], APP_CONFIG["secondary_color"]]
        
        for i, (metric_name, metric_value) in enumerate(metrics.items()):
            if metric_name == "RÂ² Score":
                display_val = f"{metric_value:.4f}"
            elif "%" in metric_name or metric_name == "MAPE":
                display_val = f"{metric_value:.1f}%"
            else:
                display_val = f"{metric_value:,.2f}"
            
            metric_cards.append(
                ui.div(
                    {"style": f"background: {colors[i % len(colors)]}; color: white; padding: 20px; border-radius: 8px; margin: 10px;"},
                    ui.div(metric_name, style="font-size: 0.9rem; opacity: 0.9;"),
                    ui.div(display_val, style="font-size: 2rem; font-weight: bold; margin-top: 10px;")
                )
            )
        
        return ui.div(
            {"style": "display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 10px;"},
            *metric_cards
        )
    
    @output
    @render_widget
    def model_comparison_plot():
        """Compare different models"""
        models = ["Gradient Boosting", "Random Forest", "XGBoost"]
        r2_scores = [0.8432, 0.8423, 0.8292]
        rmse_values = [8460.83, 8486.05, 8830.47]
        
        fig = go.Figure()
        
        fig.add_trace(go.Bar(
            name="RÂ² Score",
            x=models,
            y=r2_scores,
            yaxis="y",
            offsetgroup=1,
            marker_color=APP_CONFIG["success_color"]
        ))
        
        fig.add_trace(go.Bar(
            name="RMSE",
            x=models,
            y=rmse_values,
            yaxis="y2",
            offsetgroup=2,
            marker_color=APP_CONFIG["warning_color"]
        ))
        
        fig.update_layout(
            title="Model Performance Comparison",
            xaxis=dict(title="Model"),
            yaxis=dict(title="RÂ² Score", side="left"),
            yaxis2=dict(title="RMSE", side="right", overlaying="y"),
            barmode="group",
            height=400
        )
        
        return fig
    
    @output
    @render.ui
    def retraining_status():
        """Display model retraining status"""
        return ui.p("Upload training data and click 'Retrain Model' to begin", 
                   style="color: #666;")
    
    # =============================================================================================================
    # EXPLAINABILITY & COMPLIANCE TAB
    # =============================================================================================================
    
    @output
    @render_widget
    def global_shap_plot():
        """Global SHAP summary plot"""
        # Sample test data
        test_data_sampled = test_data.sample(n = 2000, random_state = 42, replace = False)
        # Create the explainer. Usine TreeExplainer for faster computation, seeking global performance only
        explainer = shap.TreeExplainer(
            model=final_model.named_steps["model"],
            data=final_model.named_steps["preprocessing"].transform(train_data)
        )

        # Preprocess the test data 
        preprocessed_test = final_model.named_steps["preprocessing"].transform(test_data_sampled)

        # Compute SHAP values for the test set
        shap_values = explainer(preprocessed_test)

        # Compute global feature importances as mean absolute SHAP values (in log scale)
        importances = np.abs(shap_values.values).mean(axis=0)

        # Get feature names
        feature_names = final_model.named_steps["preprocessing"].get_feature_names_out()

        # Sort features by importance descending
        indices = np.argsort(-importances)
        sorted_feature_names = feature_names[indices]
        sorted_importances = importances[indices]

        # Create Plotly bar chart for feature importances
        fig = go.Figure(go.Bar(
            x=sorted_importances,
            y=sorted_feature_names,
            orientation='h',  # Horizontal bars for better label visibility
            marker_color='rgb(158,202,225)',  # Example color
            marker_line_color='rgb(8,48,107)',
            marker_line_width=1.5,
            opacity=0.6
        ))

        fig.update_layout(
            title="SHAP Global Feature Importances (Mean Absolute SHAP Value in Log Scale)",
            xaxis_title="Mean |SHAP Value| (Importance)",
            yaxis_title="Features",
            yaxis={'categoryorder':'total ascending'},  # Sort y-axis by importance
            height=600,  # Adjust height for many features
            margin=dict(l=150)  # More space for y-labels
        )

        return fig
    
    @output
    @render.ui
    def feature_impact_summary():
        """Summary of feature impacts"""
        features_info = []
        
        for feature_key, context in list(FEATURE_BUSINESS_CONTEXT.items())[:5]:
            features_info.append(
                ui.div(
                    {"style": "border-left: 3px solid var(--admiral-orange); padding: 10px; margin: 10px 0; background: #f8f9fa;"},
                    ui.p(ui.tags.strong(context["name"]), style="margin: 0; color: var(--admiral-navy);"),
                    ui.p(f"Impact: {context['business_impact']}", style="margin: 5px 0; font-size: 0.9rem;"),
                    ui.p(f"Action: {context['actionable_insight']}", style="margin: 5px 0; font-size: 0.85rem; color: #666;")
                )
            )
        
        return ui.div(*features_info)
    
    # Your original fitted tree-based model (predicts in log-space)
    def predict_original_scale(X):
        log_pred = final_model.predict(X)
        return np.expm1(log_pred)  # returns log(y) values

    # Precompute RHALE/ALE effects once (cache for Shiny performance)
    @reactive.Calc
    def effect_cache():
        feature_names = X_train.columns.tolist() # features to analyze
        # Use a sample for speed (increase if accuracy needed)
        X_sample = X_train.sample(n=min(3000, len(X_train)), random_state=42)
        
        results = {}
        for feat_idx, feat_name in enumerate(feature_names):
            try:
                # For tree models â†’ prefer ALE (non-diff), but RHALE if you have jac (jacobian)
                # Here we use ALE as base; switch to effector.RHALE if differentiable
                method = effector.ALE(
                    data=X_sample.values,               # numpy array
                    model=lambda x: predict_original_scale,  # black-box callable
                    axis_limits=np.array([X_sample.min(), X_sample.max()]).T,
                    nof_instances="all" )
                
                # Fit the effect for this feature
                method.fit(
                    features=feat_idx,
                    centering=True,                     # center at 0
                    points_for_centering=50             # grid points
                )
                
                results[feat_name] = method
            except Exception as e:
                print(f"Effector failed for {feat_name}: {e}")
                results[feat_name] = None
        
        return results

    @reactive.Effect
    @reactive.event(session)
    def update_dropdown():
        effects = effect_cache()
        valid_features = [f for f, m in effects.items() if m is not None]
        input.selected_feature.set_choices(valid_features)

    @output
    @render_widget
    def lime_explanation_plot():
        """LIME explanation for selected claim"""
        feat = input.selected_feature()
        if not feat:
            return go.Figure(layout_title_text="Select a feature...")
        
        method = effect_cache().get(feat)
        if method is None:
            return go.Figure(layout_title_text=f"Computation failed for {feat}")
        
        # Extract data from fitted method (effector provides .data)
        effect_data = method.data[0]  # first (only) feature
        
        # effect_data is a dict-like with keys like 'xs', 'mean_effect', 'std', etc.
        xs = effect_data['xs']                        # feature grid values
        mean_effect = effect_data['mean_effect']      # average ALE/RHALE effect
        heterogeneity = effect_data.get('std', None)  # or 'bin_std' / heterogeneity measure
        
        fig = go.Figure()
        
        # Main average effect trace
        fig.add_trace(go.Scatter(
            x=xs,
            y=mean_effect,
            mode='lines+markers',
            name='Average Effect (ALE/RHALE)',
            line=dict(color='royalblue', width=2.5),
            marker=dict(size=6)
        ))
        
        # Heterogeneity band (if available)
        if heterogeneity is not None:
            lower = mean_effect - heterogeneity
            upper = mean_effect + heterogeneity
            
            fig.add_trace(go.Scatter(
                x=np.concatenate([xs, xs[::-1]]),
                y=np.concatenate([upper, lower[::-1]]),
                fill='toself',
                fillcolor='rgba(31, 119, 180, 0.18)',
                line=dict(color='rgba(255,255,255,0)'),
                name='Heterogeneity (Â± std)',
                showlegend=True
            ))
        
        # Optional: add rug or points if sample size info available
        # effector may provide 'nof_points' or similar in data
        
        fig.update_layout(
            title=f"Robust & Heterogeneity-aware ALE (RHALE) for '{feat}'",
            xaxis_title=feat,
            yaxis_title="Centered Effect",
            hovermode="x unified",
            showlegend=True,
            height=520,
            margin=dict(l=70, r=40, t=90, b=70),
            template="plotly_white"
        )
        
        return fig
    
    @output
    @render.ui
    def compliance_report():
        """Generate compliance report"""
        report_items = [
            ("Model Transparency", "âœ“", "SHAP and LIME explanations available for all predictions"),
            ("Audit Trail", "âœ“", "All predictions logged with timestamps and user IDs"),
            ("Data Privacy", "âœ“", "GDPR-compliant data handling and storage"),
            ("Bias Monitoring", "âœ“", "Regular fairness audits across protected characteristics"),
            ("Version Control", "âœ“", "All model versions tracked in MLflow Registry")
        ]
        
        report_elements = []
        for item, status, description in report_items:
            report_elements.append(
                ui.div(
                    {"style": "padding: 15px; border-bottom: 1px solid #dee2e6;"},
                    ui.p(
                        ui.tags.strong(f"{status} {item}"),
                        style="margin: 0; color: #28a745;"
                    ),
                    ui.p(description, style="margin: 5px 0 0 0; font-size: 0.9rem; color: #666;")
                )
            )
        
        return ui.div(*report_elements)
    
    # ========================================================================
    # BUSINESS ANALYTICS TAB
    # ========================================================================
    
    @output
    @render.ui
    def kpi_prediction_accuracy():
        """KPI: Prediction accuracy"""
        result =uploaded_data()
        # Calculate correlation and square it
        r_squared = r2_score(result['Ultimate_Claim_Amount'],
                            result['Predictions'])
        r_squared = r_squared * 100
        return ui.div(
            {"class": "metric-card"},
            ui.div("Prediction Accuracy", {"class": "metric-label"}),
            ui.div(f"{r_squared:.2f}%", {"class": "metric-value"})
        )
    
    @output
    @render.ui
    def kpi_processing_time():
        """KPI: Average processing time"""
        result = uploaded_data()
        process_diff = result['Settlement_Date'] - result['Accident_Date']
        process_diff = process_diff.dt.days
        mean_time = process_diff.mean()
        return ui.div(
            {"class": "metric-card"},
            ui.div("Avg Processing Time", {"class": "metric-label"}),
            ui.div(f"{mean_time:.1f}days", {"class": "metric-value"})
        )
    
    @output
    @render.ui
    def kpi_cost_savings():
        """KPI: Cost savings"""
        return ui.div(
            {"class": "metric-card"},
            ui.div("Monthly Savings", {"class": "metric-label"}),
            ui.div("Â£127K", {"class": "metric-value"})
        )
    
    @output
    @render.ui
    def kpi_model_confidence():
        """KPI: Model confidence"""
        return ui.div(
            {"class": "metric-card"},
            ui.div("Model Confidence", {"class": "metric-label"}),
            ui.div("High", {"class": "metric-value"})
        )
    
    @output
    @render_widget
    def claims_trend_plot():
        """Claims trend over time"""
        result = uploaded_data()

        # Create a 'Month' column (Period format 'YYYY-MM' is best for sorting)
        result['Month'] = result['Accident_Date'].dt.to_period('M')
        result['Month'] = result['Month'].dt.to_timestamp()

        # Group and calculate Volume and Total Amount
        monthly_summary = result.groupby('Month').agg(
            Policy_Volume=('Policy_ID', 'nunique'),          # Counts unique policies
            Total_Disbursed=('Ultimate_Claim_Amount', 'sum') # Sums the claim amounts
        ).reset_index()

        fig = go.Figure()
        
        fig.add_trace(go.Scatter(
            x=monthly_summary["Month"],
            y=monthly_summary["Policy_Volume"],
            name="Claim Volume",
            yaxis="y",
            line=dict(color=APP_CONFIG["primary_color"])
        ))
        
        fig.add_trace(go.Scatter(
            x=monthly_summary["Month"],
            y=monthly_summary["Total_Disbursed"],
            name="Avg Claim Amount",
            yaxis="y2",
            line=dict(color=APP_CONFIG["secondary_color"])
        ))
        
        fig.update_layout(
            title="Claims Volume and Average Amount Trends",
            xaxis=dict(title="Month"),
            yaxis=dict(title="Volume", side="left"),
            yaxis2=dict(title="Amount (Â£)", side="right", overlaying="y"),
            height=400
        )
        
        return fig
    
    @output
    @render_widget
    def reserve_adequacy_plot():
        """Reserve adequacy analysis"""
        categories = ["Adequate", "Under-Reserved", "Over-Reserved"]
        percentages = [75, 15, 10]
        
        fig = px.pie(
            values=percentages,
            names=categories,
            title="Reserve Adequacy Distribution",
            color=categories,
            color_discrete_map={
                "Adequate": APP_CONFIG["success_color"],
                "Under-Reserved": APP_CONFIG["danger_color"],
                "Over-Reserved": APP_CONFIG["warning_color"]
            }
        )
        
        fig.update_layout(height=400)
        return fig
    
    @output
    @render.ui
    def strategic_insights():
        """Strategic business insights"""
        insights = [
            {
                "title": "Reserve Optimization Opportunity",
                "description": "ML predictions show 15% improvement in reserve accuracy compared to manual estimates, potentially saving Â£1.2M annually.",
                "priority": "High"
            },
            {
                "title": "Fraud Detection Enhancement",
                "description": "Claims with >24hr FNOL delays show 40% higher final costs. Implement automated early warning system.",
                "priority": "High"
            },
            {
                "title": "Customer Experience Improvement",
                "description": "Faster predictions enable 30% reduction in settlement times for low-severity claims.",
                "priority": "Medium"
            }
        ]
        
        insight_elements = []
        for insight in insights:
            priority_color = APP_CONFIG["danger_color"] if insight["priority"] == "High" else APP_CONFIG["warning_color"]
            
            insight_elements.append(
                ui.div(
                    {"style": f"border-left: 4px solid {priority_color}; padding: 20px; margin: 15px 0; background: #f8f9fa; border-radius: 5px;"},
                    ui.h5(insight["title"], style="color: var(--admiral-navy); margin-top: 0;"),
                    ui.p(insight["description"]),
                    ui.tags.span(
                        f"Priority: {insight['priority']}",
                        style=f"background: {priority_color}; color: white; padding: 3px 10px; border-radius: 3px; font-size: 0.85rem;"
                    )
                )
            )
        
        return ui.div(*insight_elements)
